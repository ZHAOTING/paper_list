# Papers on NLG evaluation

## 2020

BLEURT: Learning Robust Metrics for Text Generation, Sellam et al., ACL 2020.
[arxiv](https://arxiv.org/abs/2004.04696)

USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation, Mehri and Eskenazi, ACL 2020.
[arxiv](https://arxiv.org/abs/2005.00456)

Learning an Unreferenced Metric for Online Dialogue Evaluation, Sinha et al., ACL 2020.
[arxiv](https://arxiv.org/abs/2005.00583)

Evaluating Dialogue Generation Systems via Response Selection, Sato et al., ACL 2020.
[arxiv](https://arxiv.org/abs/2004.14302)

Designing Precise and Robust Dialogue Response Evaluators, Zhao et al., ACL 2020.
[arxiv](https://arxiv.org/abs/2004.04908), [code](https://github.com/ZHAOTING/dialog-processing)

## 2019

Unifying Human and Statistical Evaluation for Natural Language Generation, Hashimoto et al., NAACL 2019.
[arxiv](https://arxiv.org/abs/1904.02792)

Better Automatic Evaluation of Open-Domain Dialogue Systems with Contextualized Embeddings, Ghazarian et al., NAACL 2019 NeuralGen WS.
[arxiv](https://arxiv.org/abs/1904.10635)

Evaluating Coherence in Dialogue Systems using Entailment, Dziri et al., NAACL 2019.
[arxiv](https://arxiv.org/abs/1904.03371)

Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts, Clark et al., ACL 2019.
[ACL anthology](https://www.aclweb.org/anthology/P19-1264/)

Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems, Ghandeharioun et al., NeurIPS 2019.
[arxiv](https://arxiv.org/abs/1906.09308)

ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons, Li et al., arxiv 2019.
[arxiv](https://arxiv.org/abs/1909.03087)

Towards Coherent and Engaging Spoken Dialog Response Generation Using Automatic Conversation Evaluators, Yi et al., INLG 2019.
[ACL anthology](https://www.aclweb.org/anthology/W19-8608/)

Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement, Deriu and Cieliebak, INLG 2019.
[arxiv](https://arxiv.org/abs/1909.12066)

Re-evaluating ADEM: A Deeper Look at Scoring Dialogue Responses, Sai et al., AAAI 2019.
[arxiv](https://arxiv.org/abs/1902.08832)

Investigating Evaluation of Open-Domain Dialogue Systems With Human Generated Multiple References, Gupta et al., SIGDIAL 2019.
[arxiv](https://arxiv.org/abs/1907.10568)

Importance of Search and Evaluation Strategies in Neural Dialogue Modeling, Kulikov et al., INLG 2019.
[arxiv](https://arxiv.org/abs/1811.00907)

## 2017

Topic-based Evaluation for Conversational Bots, Guo et al., NIPS 2017 ConvAI WS.
[arxiv](https://arxiv.org/abs/1801.03622)

Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses, Lowe et al., ACL 2017.
[arxiv](https://arxiv.org/abs/1708.07149)

Adversarial Evaluation of Dialogue Models, Kannan and Vinyals, arxiv 2017.
[arxiv](https://arxiv.org/abs/1701.08198)

Adversarial evaluation for open-domain dialogue generation, Bruni and Fernandez, SIGDIAL 2017.
[ACL anthology](https://www.aclweb.org/anthology/W17-5534/)

RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems, Tao et al., arxiv 2017.
[arxiv](https://arxiv.org/abs/1701.03079)

Why We Need New Evaluation Metrics for NLG, Novikova et al., EMNLP 2017.
[arxiv](https://arxiv.org/abs/1707.06875)

Sentence-Level Fluency Evaluation: References Help, But Can Be Spared!, Kann et al., CoNLL 2018.
[arxiv](https://arxiv.org/abs/1809.08731)




## 2016

How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation, Liu et al., EMNLP 2016.
[arxiv](https://arxiv.org/abs/1603.08023)